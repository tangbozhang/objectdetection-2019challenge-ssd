{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**简介**\n",
    "\n",
    "Single Shot MultiBox Detector (SSD) 是一种单阶段的目标检测器。与两阶段的检测方法不同，单阶段目标检测并不进行区域推荐，而是直接从特征图回归出目标的边界框和分类概率。SSD 运用了这种单阶段检测的思想，并且对其进行改进：在不同尺度的特征图上检测对应尺度的目标。如下图所示，SSD 在六个尺度的特征图上进行了不同层级的预测。每个层级由两个3x3卷积分别对目标类别和边界框偏移进行回归。因此对于每个类别，SSD 的六个层级一共会产生 38x38x4 + 19x19x6 + 10x10x6 + 5x5x6 + 3x3x4 + 1x1x4 = 8732 个检测结果。\n",
    "\n",
    "\n",
    "\n",
    "SSD 可以方便地插入到任何一种标准卷积网络中，比如 VGG、ResNet 或者 MobileNet，这些网络被称作检测器的基网络。在这个示例中我们使用 shufflenet。\n",
    "\n",
    "在训练时还会对图片进行数据增强，包括随机扰动、扩张、翻转和裁剪:\n",
    "\n",
    "* 扰动: 扰动图片亮度、对比度、饱和度和色相。\n",
    "* 扩张: 将原始图片放进一张使用像素均值填充(随后会在减均值操作中减掉)的扩张图中，再对此图进行裁剪、缩放和翻转。\n",
    "* 翻转: 水平翻转。\n",
    "* 裁剪: 根据缩放比例、长宽比例两个参数生成若干候选框，再依据这些候选框和标注框的面积交并比(IoU)挑选出符合要求的裁剪结果。\n",
    "\n",
    "也可以采用一些其他数据增广方法：\n",
    "\n",
    "**InstaBoost:**\n",
    "\n",
    "论文： https://arxiv.org/abs/1908.07801\n",
    "\n",
    "代码： https://github.com/GothicAi/InstaBoost\n",
    "\n",
    "\n",
    "**RandAugment：**\n",
    "\n",
    "https://arxiv.org/pdf/1909.13719.pdf\n",
    "\n",
    "https://github.com/tensorflow/tpu/blob/master/models/official/efficientnet/autoaugment.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**目录**\n",
    "```\n",
    "| work\n",
    "   |-- astar2019\n",
    "      |-- score.py\n",
    "      |-- ...\n",
    "   |-- ssd\n",
    "      |-- train.py\n",
    "      |-- mobilenet_ssd.py\n",
    "      |-- ...\n",
    "   |-- coco\n",
    "      |-- train2017\n",
    "      |-- val2017\n",
    "      |-- test2017\n",
    "      |-- ...\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**安装相关依赖库**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python trian/train.py --learning_rate 0.008 --data_dir F:\\imagesource\\coco\\coco2017 --epoc_num 1 --batch_size 32 --model_save_dir model/model_snet --pretrained_model model\\pretrained\\ShuffleNetV2_pretrained --use_multiprocess False --enable_ce True\n",
    "\n",
    "python astar2019-evauate/score.py --batch_size 32 --data_dir F:\\imagesource\\coco\\coco2017 --model_dir model/model_snet/300_300_inference/  \n",
    "    \n",
    "python trian/main_quant.py  --batch_size 16 --data_dir F:\\imagesource\\coco\\coco2017 --epoc_num 1 --init_model model/model_snet/best_model --model_save_dir model/model_snet/snet_int8 --mode train\n",
    "   \n",
    "python astar2019-evauate/score.py --batch_size 16 --data_dir F:\\imagesource\\coco\\coco2017 --model_dir model/model_snet/snet_int8/300_300_int8_inference/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paddle已发布了图像分类模型库\n",
    "\n",
    "https://github.com/PaddlePaddle/models/tree/develop/PaddleCV/image_classification\n",
    "\n",
    "用户可以参照work/ssd/中的mobilenet_ssd、shufflenetv2_ssd.py、mobilenetv2_ssd.py等添加自己的backbone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**开始训练**\n",
    "\n",
    "训练策略采用了warmup，优化器采用的Momentum，用户也可以自己更改优化器\n",
    "保存的预测模型会以图片的height_width_xxx来命名"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义路径   \n",
    "!python trian/train.py --learning_rate 0.008 --data_dir F:\\imagesource\\coco\\coco2017 --epoc_num 1 --batch_size 16 --model_save_dir model/model_snet --pretrained_model model\\pretrained\\ShuffleNetV2_pretrained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "执行score\n",
    "score模型的名字需要是height_width_xxx，默认300x300\n",
    "\n",
    "--model_dir 预测模型地址"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------  Configuration Arguments -----------\n",
      "ap_version: cocoMAP\n",
      "batch_size: 16\n",
      "data_dir: dataset/coco2017\n",
      "mean_value_B: 127.5\n",
      "mean_value_G: 127.5\n",
      "mean_value_R: 127.5\n",
      "model_dir: model/model_snet/300_300_inference/\n",
      "nms_threshold: 0.45\n",
      "test_list: \n",
      "------------------------------------------------\n",
      "loading annotations into memory...\n",
      "Done (t=0.60s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.62s)\n",
      "creating index...\n",
      "index created!\n",
      "json_category_id_to_contiguous_id =  {1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10, 11: 11, 13: 12, 14: 13, 15: 14, 16: 15, 17: 16, 18: 17, 19: 18, 20: 19, 21: 20, 22: 21, 23: 22, 24: 23, 25: 24, 27: 25, 28: 26, 31: 27, 32: 28, 33: 29, 34: 30, 35: 31, 36: 32, 37: 33, 38: 34, 39: 35, 40: 36, 41: 37, 42: 38, 43: 39, 44: 40, 46: 41, 47: 42, 48: 43, 49: 44, 50: 45, 51: 46, 52: 47, 53: 48, 54: 49, 55: 50, 56: 51, 57: 52, 58: 53, 59: 54, 60: 55, 61: 56, 62: 57, 63: 58, 64: 59, 65: 60, 67: 61, 70: 62, 72: 63, 73: 64, 74: 65, 75: 66, 76: 67, 77: 68, 78: 69, 79: 70, 80: 71, 81: 72, 82: 73, 84: 74, 85: 75, 86: 76, 87: 77, 88: 78, 89: 79, 90: 80}\n",
      "contiguous_category_id_to_json_id =  {1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10, 11: 11, 12: 13, 13: 14, 14: 15, 15: 16, 16: 17, 17: 18, 18: 19, 19: 20, 20: 21, 21: 22, 22: 23, 23: 24, 24: 25, 25: 27, 26: 28, 27: 31, 28: 32, 29: 33, 30: 34, 31: 35, 32: 36, 33: 37, 34: 38, 35: 39, 36: 40, 37: 41, 38: 42, 39: 43, 40: 44, 41: 46, 42: 47, 43: 48, 44: 49, 45: 50, 46: 51, 47: 52, 48: 53, 49: 54, 50: 55, 51: 56, 52: 57, 53: 58, 54: 59, 55: 60, 56: 61, 57: 62, 58: 63, 59: 64, 60: 65, 61: 67, 62: 70, 63: 72, 64: 73, 65: 74, 66: 75, 67: 76, 68: 77, 69: 78, 70: 79, 71: 80, 72: 81, 73: 82, 74: 84, 75: 85, 76: 86, 77: 87, 78: 88, 79: 89, 80: 90}\n",
      "boxes.shape =  (-1, -1, 4)\n",
      "scores.shape =  (-1, -1, 81)\n",
      "Batch 0\n",
      "Batch 20\n",
      "Batch 40\n",
      "Batch 60\n",
      "Batch 80\n",
      "Batch 100\n",
      "Batch 120\n",
      "Batch 140\n",
      "Batch 160\n",
      "Batch 180\n",
      "Batch 200\n",
      "Batch 220\n",
      "Batch 240\n",
      "Batch 260\n",
      "Batch 280\n",
      "Batch 300\n",
      "start evaluate using coco api\n",
      "Loading and preparing results...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\softwareinstall\\deeplearn\\anaconda3\\envs\\paddle\\lib\\site-packages\\paddle\\fluid\\executor.py:790: UserWarning: The current program is empty.\n",
      "  warnings.warn(error_info)\n",
      "Traceback (most recent call last):\n",
      "  File \"astar2019-evauate/score.py\", line 187, in <module>\n",
      "    score, mAP, flops = compute_score(args.model_dir, args.data_dir, batch_size=args.batch_size)\n",
      "  File \"astar2019-evauate/score.py\", line 161, in compute_score\n",
      "    feeded_var_names, feeder, target_var, batch_size)\n",
      "  File \"astar2019-evauate/score.py\", line 86, in use_coco_api_compute_mAP\n",
      "    cocoDt = cocoGt.loadRes(tmp_file)\n",
      "  File \"D:\\softwareinstall\\deeplearn\\anaconda3\\envs\\paddle\\lib\\site-packages\\pycocotools\\coco.py\", line 316, in loadRes\n",
      "    anns = json.load(open(resFile))\n",
      "  File \"D:\\softwareinstall\\deeplearn\\anaconda3\\envs\\paddle\\lib\\json\\__init__.py\", line 293, in load\n",
      "    return loads(fp.read(),\n",
      "MemoryError\n",
      "W0114 15:56:20.981066  4908 device_context.cc:235] Please NOTE: device: 0, CUDA Capability: 61, Driver API Version: 10.2, Runtime API Version: 10.0\n",
      "W0114 15:56:20.986032  4908 device_context.cc:243] device: 0, cuDNN Version: 7.3.\n"
     ]
    }
   ],
   "source": [
    "!python astar2019-evauate/score.py --batch_size 8 --data_dir F:\\imagesource\\coco\\coco2017 --model_dir model/model_snet/300_300_inference/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**量化训练**\n",
    "\n",
    "此部分用的是main_quant.py里的量化方法\n",
    "\n",
    "也可以用PaddleSlim里的方法，详见work/paddleslim，同时PaddleSlim也可以进行剪枝等操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python trian/main_quant.py  --data_dir F:\\imagesource\\coco\\coco2017 --init_model model/model_snet/best_model --model_save_dir model/model_snet/snet_int8 --mode train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "执行量化后的评测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------  Configuration Arguments -----------\n",
      "ap_version: cocoMAP\n",
      "batch_size: 16\n",
      "data_dir: dataset/coco2017\n",
      "mean_value_B: 127.5\n",
      "mean_value_G: 127.5\n",
      "mean_value_R: 127.5\n",
      "model_dir: model/model_snet/snet_int8/300_300_int8_inference\n",
      "nms_threshold: 0.45\n",
      "test_list: \n",
      "------------------------------------------------\n",
      "height =  300\n",
      "width =  300\n",
      "loading annotations into memory...\n",
      "Done (t=0.61s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.75s)\n",
      "creating index...\n",
      "index created!\n",
      "json_category_id_to_contiguous_id =  {1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10, 11: 11, 13: 12, 14: 13, 15: 14, 16: 15, 17: 16, 18: 17, 19: 18, 20: 19, 21: 20, 22: 21, 23: 22, 24: 23, 25: 24, 27: 25, 28: 26, 31: 27, 32: 28, 33: 29, 34: 30, 35: 31, 36: 32, 37: 33, 38: 34, 39: 35, 40: 36, 41: 37, 42: 38, 43: 39, 44: 40, 46: 41, 47: 42, 48: 43, 49: 44, 50: 45, 51: 46, 52: 47, 53: 48, 54: 49, 55: 50, 56: 51, 57: 52, 58: 53, 59: 54, 60: 55, 61: 56, 62: 57, 63: 58, 64: 59, 65: 60, 67: 61, 70: 62, 72: 63, 73: 64, 74: 65, 75: 66, 76: 67, 77: 68, 78: 69, 79: 70, 80: 71, 81: 72, 82: 73, 84: 74, 85: 75, 86: 76, 87: 77, 88: 78, 89: 79, 90: 80}\n",
      "contiguous_category_id_to_json_id =  {1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9, 10: 10, 11: 11, 12: 13, 13: 14, 14: 15, 15: 16, 16: 17, 17: 18, 18: 19, 19: 20, 20: 21, 21: 22, 22: 23, 23: 24, 24: 25, 25: 27, 26: 28, 27: 31, 28: 32, 29: 33, 30: 34, 31: 35, 32: 36, 33: 37, 34: 38, 35: 39, 36: 40, 37: 41, 38: 42, 39: 43, 40: 44, 41: 46, 42: 47, 43: 48, 44: 49, 45: 50, 46: 51, 47: 52, 48: 53, 49: 54, 50: 55, 51: 56, 52: 57, 53: 58, 54: 59, 55: 60, 56: 61, 57: 62, 58: 63, 59: 64, 60: 65, 61: 67, 62: 70, 63: 72, 64: 73, 65: 74, 66: 75, 67: 76, 68: 77, 69: 78, 70: 79, 71: 80, 72: 81, 73: 82, 74: 84, 75: 85, 76: 86, 77: 87, 78: 88, 79: 89, 80: 90}\n",
      "boxes.shape =  (-1, -1, 4)\n",
      "scores.shape =  (-1, -1, 81)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\softwareinstall\\deeplearn\\anaconda3\\envs\\paddle\\lib\\site-packages\\paddle\\fluid\\executor.py:790: UserWarning: The current program is empty.\n",
      "  warnings.warn(error_info)\n",
      "D:\\softwareinstall\\deeplearn\\anaconda3\\envs\\paddle\\lib\\site-packages\\paddle\\fluid\\executor.py:774: UserWarning: The following exception is not an EOF exception.\n",
      "  \"The following exception is not an EOF exception.\")\n",
      "Traceback (most recent call last):\n",
      "  File \"astar2019-evauate/score.py\", line 187, in <module>\n",
      "    score, mAP, flops = compute_score(args.model_dir, args.data_dir, batch_size=args.batch_size)\n",
      "  File \"astar2019-evauate/score.py\", line 161, in compute_score\n",
      "    feeded_var_names, feeder, target_var, batch_size)\n",
      "  File \"astar2019-evauate/score.py\", line 70, in use_coco_api_compute_mAP\n",
      "    fetch_list=target_var)\n",
      "  File \"D:\\softwareinstall\\deeplearn\\anaconda3\\envs\\paddle\\lib\\site-packages\\paddle\\fluid\\executor.py\", line 775, in run\n",
      "    six.reraise(*sys.exc_info())\n",
      "  File \"D:\\softwareinstall\\deeplearn\\anaconda3\\envs\\paddle\\lib\\site-packages\\six.py\", line 696, in reraise\n",
      "    raise value\n",
      "  File \"D:\\softwareinstall\\deeplearn\\anaconda3\\envs\\paddle\\lib\\site-packages\\paddle\\fluid\\executor.py\", line 770, in run\n",
      "    use_program_cache=use_program_cache)\n",
      "  File \"D:\\softwareinstall\\deeplearn\\anaconda3\\envs\\paddle\\lib\\site-packages\\paddle\\fluid\\executor.py\", line 817, in _run_impl\n",
      "    use_program_cache=use_program_cache)\n",
      "  File \"D:\\softwareinstall\\deeplearn\\anaconda3\\envs\\paddle\\lib\\site-packages\\paddle\\fluid\\executor.py\", line 894, in _run_program\n",
      "    fetch_var_name)\n",
      "RuntimeError: \n",
      "\n",
      "--------------------------------------------\n",
      "C++ Call Stacks (More useful to developers):\n",
      "--------------------------------------------\n",
      "Windows not support stack backtrace yet.\n",
      "\n",
      "----------------------\n",
      "Error Message Summary:\n",
      "----------------------\n",
      "PaddleCheckError: \n",
      "\n",
      "Out of memory error on GPU 0. Cannot allocate 4.248291MB memory on GPU 0, available memory is only 2.137499MB.\n",
      "\n",
      "Please check whether there is any other process using GPU 0.\n",
      "1. If yes, please stop them, or start PaddlePaddle on another GPU.\n",
      "2. If no, please try one of the following suggestions:\n",
      "   1) Decrease the batch size of your model.\n",
      "   2) FLAGS_fraction_of_gpu_memory_to_use is 0.50 now, please set it to a higher value but less than 1.0.\n",
      "      The command is `export FLAGS_fraction_of_gpu_memory_to_use=xxx`.\n",
      "\n",
      " at [D:\\1.6.1\\paddle\\paddle\\fluid\\memory\\detail\\system_allocator.cc:151]\n",
      "\n",
      "W0114 14:29:21.145592  8424 device_context.cc:235] Please NOTE: device: 0, CUDA Capability: 61, Driver API Version: 10.2, Runtime API Version: 10.0\n",
      "W0114 14:29:21.151055  8424 device_context.cc:243] device: 0, cuDNN Version: 7.3.\n",
      "W0114 14:29:36.890214  8424 operator.cc:179] fake_dequantize_max_abs raises an exception struct paddle::memory::allocation::BadAlloc, \n",
      "\n",
      "--------------------------------------------\n",
      "C++ Call Stacks (More useful to developers):\n",
      "--------------------------------------------\n",
      "Windows not support stack backtrace yet.\n",
      "\n",
      "----------------------\n",
      "Error Message Summary:\n",
      "----------------------\n",
      "PaddleCheckError: \n",
      "\n",
      "Out of memory error on GPU 0. Cannot allocate 4.248291MB memory on GPU 0, available memory is only 2.137499MB.\n",
      "\n",
      "Please check whether there is any other process using GPU 0.\n",
      "1. If yes, please stop them, or start PaddlePaddle on another GPU.\n",
      "2. If no, please try one of the following suggestions:\n",
      "   1) Decrease the batch size of your model.\n",
      "   2) FLAGS_fraction_of_gpu_memory_to_use is 0.50 now, please set it to a higher value but less than 1.0.\n",
      "      The command is `export FLAGS_fraction_of_gpu_memory_to_use=xxx`.\n",
      "\n",
      " at [D:\\1.6.1\\paddle\\paddle\\fluid\\memory\\detail\\system_allocator.cc:151]\n"
     ]
    }
   ],
   "source": [
    "!python astar2019-evauate/score.py --batch_size 32 --data_dir F:\\imagesource\\coco\\coco2017 --model_dir model/model_snet/snet_int8/300_300_int8_inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型提交"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'rm' 不是内部或外部命令，也不是可运行的程序\n",
      "或批处理文件。\n",
      "'wget' 不是内部或外部命令，也不是可运行的程序\n",
      "或批处理文件。\n",
      "'sh' 不是内部或外部命令，也不是可运行的程序\n",
      "或批处理文件。\n"
     ]
    }
   ],
   "source": [
    "#模型提交指令\n",
    "!rm -rf submit.sh\n",
    "!wget -O submit.sh http://ai-studio-static.bj.bcebos.com/script/submit.sh\n",
    "!sh submit.sh model/model_snet/300_300_SSD.zip 18a12c8188884304b3f638f143c90047"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'rm' 不是内部或外部命令，也不是可运行的程序\n",
      "或批处理文件。\n",
      "'wget' 不是内部或外部命令，也不是可运行的程序\n",
      "或批处理文件。\n",
      "'sh' 不是内部或外部命令，也不是可运行的程序\n",
      "或批处理文件。\n"
     ]
    }
   ],
   "source": [
    "#模型提交2\n",
    "!cd model/model_snet/snet_int8/ && rm -rf submit.sh\n",
    "!cd model/model_snet/snet_int8/ && wget -O submit.sh http://ai-studio-static.bj.bcebos.com/script/submit.sh\n",
    "!cd model/model_snet/snet_int8/ && sh submit.sh 300_300_SSD.zip token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
